# -*- coding: utf-8 -*-
"""R_mathematical_model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wnhdo78L5HAjwIC7IS9pn1atn73BTt_2
"""

pip install pandas scikit-learn matplotlib seaborn

from google.colab import files
files.upload()

import pandas as pd

# Load the datasets
red_wine = pd.read_csv('winequality-red.csv', sep=';')
white_wine = pd.read_csv('winequality-white.csv', sep=';')

# Display the first few rows of the datasets
print(red_wine.head())
print(white_wine.head())

# Check for missing values
print(red_wine.isnull().sum())
print(white_wine.isnull().sum())

# Drop any non-numeric columns if necessary (in this case, there are none)

#EXPT 5 - K-MEANS CLUSTERING
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt

# Choose the number of clusters
n_clusters = 3  # You can adjust this

# Fit K-means for red wine
kmeans_red = KMeans(n_clusters=n_clusters)
red_wine['kmeans_cluster'] = kmeans_red.fit_predict(red_wine)

# Fit K-means for white wine
kmeans_white = KMeans(n_clusters=n_clusters)
white_wine['kmeans_cluster'] = kmeans_white.fit_predict(white_wine)

# Visualize the clusters (using two features for simplicity)
plt.figure(figsize=(12, 6))
plt.subplot(1, 2, 1)
plt.scatter(red_wine['alcohol'], red_wine['quality'], c=red_wine['kmeans_cluster'])
plt.title('K-means Clustering (Red Wine)')
plt.xlabel('Alcohol')
plt.ylabel('Quality')

plt.subplot(1, 2, 2)
plt.scatter(white_wine['alcohol'], white_wine['quality'], c=white_wine['kmeans_cluster'])
plt.title('K-means Clustering (White Wine)')
plt.xlabel('Alcohol')
plt.ylabel('Quality')

plt.show()

#MIXTURES OF GAUSSIANS
from sklearn.mixture import GaussianMixture

# Fit Gaussian Mixture Model for red wine
gmm_red = GaussianMixture(n_components=n_clusters)
red_wine['gmm_cluster'] = gmm_red.fit_predict(red_wine)

# Fit Gaussian Mixture Model for white wine
gmm_white = GaussianMixture(n_components=n_clusters)
white_wine['gmm_cluster'] = gmm_white.fit_predict(white_wine)

# Visualize the GMM clusters
plt.figure(figsize=(12, 6))
plt.subplot(1, 2, 1)
plt.scatter(red_wine['alcohol'], red_wine['quality'], c=red_wine['gmm_cluster'])
plt.title('GMM Clustering (Red Wine)')
plt.xlabel('Alcohol')
plt.ylabel('Quality')

plt.subplot(1, 2, 2)
plt.scatter(white_wine['alcohol'], white_wine['quality'], c=white_wine['gmm_cluster'])
plt.title('GMM Clustering (White Wine)')
plt.xlabel('Alcohol')
plt.ylabel('Quality')

plt.show()

#HIERARCHICAL CLUSTERING
from sklearn.cluster import AgglomerativeClustering
import seaborn as sns
import scipy.cluster.hierarchy as sch

# Fit Hierarchical Clustering for red wine
hierarchical_red = AgglomerativeClustering(n_clusters=n_clusters)
red_wine['hierarchical_cluster'] = hierarchical_red.fit_predict(red_wine)

# Fit Hierarchical Clustering for white wine
hierarchical_white = AgglomerativeClustering(n_clusters=n_clusters)
white_wine['hierarchical_cluster'] = hierarchical_white.fit_predict(white_wine)

# Visualize the dendrogram for red wine
plt.figure(figsize=(10, 7))
dendrogram_red = sch.dendrogram(sch.linkage(red_wine[['alcohol', 'quality']], method='ward'))
plt.title('Dendrogram (Red Wine)')
plt.xlabel('Samples')
plt.ylabel('Euclidean distances')
plt.show()

# Visualize the dendrogram for white wine
plt.figure(figsize=(10, 7))
dendrogram_white = sch.dendrogram(sch.linkage(white_wine[['alcohol', 'quality']], method='ward'))
plt.title('Dendrogram (White Wine)')
plt.xlabel('Samples')
plt.ylabel('Euclidean distances')
plt.show()

#EXPT 6 - CREATE A PROGRAM TO PERFORM PCA
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler

# Load the datasets
red_wine = pd.read_csv('winequality-red.csv', sep=';')
white_wine = pd.read_csv('winequality-white.csv', sep=';')

# Display the first few rows of the datasets
print("Red Wine Dataset Head:")
print(red_wine.head())
print("\nWhite Wine Dataset Head:")
print(white_wine.head())

# Normalize the data
scaler = StandardScaler()
X_red = scaler.fit_transform(red_wine.drop('quality', axis=1))  # dropping the target variable 'quality'
X_white = scaler.fit_transform(white_wine.drop('quality', axis=1))

# Perform PCA
pca_red = PCA(n_components=2)  # Reduce to 2 dimensions for visualization
X_red_pca = pca_red.fit_transform(X_red)

pca_white = PCA(n_components=2)  # Reduce to 2 dimensions for visualization
X_white_pca = pca_white.fit_transform(X_white)

# Plot the PCA results for Red Wine
plt.figure(figsize=(12, 6))

plt.subplot(1, 2, 1)
plt.scatter(X_red_pca[:, 0], X_red_pca[:, 1], alpha=0.7)
plt.title('PCA of Red Wine')
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')

# Plot the PCA results for White Wine
plt.subplot(1, 2, 2)
plt.scatter(X_white_pca[:, 0], X_white_pca[:, 1], alpha=0.7, color='orange')
plt.title('PCA of White Wine')
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')

plt.tight_layout()
plt.show()

# Explained Variance
print("Explained Variance Ratio for Red Wine:", pca_red.explained_variance_ratio_)
print("Explained Variance Ratio for White Wine:", pca_white.explained_variance_ratio_)

#EXPT 7 - IMPLEMENT HMM TO PREDICT THE SEQUENTIAL DATA
!pip install numpy pandas hmmlearn

import numpy as np
import pandas as pd
from hmmlearn import hmm
import matplotlib.pyplot as plt

# Generate synthetic sequential data for demonstration
np.random.seed(42)

# Parameters for the HMM
n_states = 2        # Number of hidden states
n_observations = 100  # Number of observed data points

# Transition probabilities
transitions = np.array([[0.7, 0.3],  # From state 0 to (0, 1)
                        [0.4, 0.6]]) # From state 1 to (0, 1)

# Emission probabilities (means for Gaussian emissions)
means = np.array([[0.0], [3.0]])        # Mean for each state

# Covariance matrix for each state as a full covariance for 1D
# Each state should have a 1x1 covariance matrix
covariances = np.array([[[0.5]], [[0.5]]])  # Shape: (n_components, n_dim, n_dim)

# Initial state probabilities
startprob = np.array([0.6, 0.4])

# Create the HMM model
model = hmm.GaussianHMM(n_components=n_states, covariance_type='full', n_iter=1000)

# Set the model parameters
model.startprob_ = startprob
model.transmat_ = transitions
model.means_ = means
model.covars_ = covariances

# Generate sample data from the HMM
X, Z = model.sample(n_observations)

# Reshape the data for training
X = X.reshape(-1, 1)  # Reshape for HMM input

# Train the HMM
model.fit(X)

# Predict the hidden states based on observations
hidden_states = model.predict(X)

# Visualize the results
plt.figure(figsize=(12, 6))
plt.title('Hidden Markov Model Example')
plt.plot(X, label='Observed Data', marker='o', linestyle='None', alpha=0.6)
plt.scatter(range(len(hidden_states)), hidden_states, label='Predicted Hidden States', color='red', marker='x', s=100)
plt.xlabel('Time Step')
plt.ylabel('Observation Value')
plt.legend()
plt.show()

# State probabilities for the observed data
print("Hidden States:\n", hidden_states)
print("\nMeans:\n", model.means_)
print("\nCovariances:\n", model.covars_)
print("\nTransition Matrix:\n", model.transmat_)
print("\nStart Probabilities:\n", model.startprob_)

from google.colab import files
files.upload()

import pandas as pd

# Load the datasets
red_wine = pd.read_csv('winequality-red.csv', sep=';')
white_wine = pd.read_csv('winequality-white.csv', sep=';')

# Display the first few rows of the datasets
print(red_wine.head())
print(white_wine.head())

# Check for missing values
print(red_wine.isnull().sum())
print(white_wine.isnull().sum())

# Drop any non-numeric columns if necessary (in this case, there are none)

#EXPT 8 - Implement CART learning algorithms to perform categorization.
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import classification_report, confusion_matrix

# Step 1: Load the datasets
red_wine = pd.read_csv('winequality-red.csv', sep=';')  # Ensure correct separator
white_wine = pd.read_csv('winequality-white.csv', sep=';')

# Step 2: Preprocess the data
# For simplicity, let's treat both datasets separately. You can concatenate or combine features later if desired.
def prepare_data(wine_data):
    # Here, we assume 'quality' is the target variable
    X = wine_data.drop(columns=['quality'])
    y = wine_data['quality']
    return X, y

X_red, y_red = prepare_data(red_wine)
X_white, y_white = prepare_data(white_wine)

# Step 3: Split the data into training and testing sets
X_train_red, X_test_red, y_train_red, y_test_red = train_test_split(X_red, y_red, test_size=0.2, random_state=42)
X_train_white, X_test_white, y_train_white, y_test_white = train_test_split(X_white, y_white, test_size=0.2, random_state=42)

# Step 4: Train CART models
cart_model_red = DecisionTreeClassifier(random_state=42)
cart_model_red.fit(X_train_red, y_train_red)

cart_model_white = DecisionTreeClassifier(random_state=42)
cart_model_white.fit(X_train_white, y_train_white)

# Step 5: Evaluate the models
# Red wine evaluation
y_pred_red = cart_model_red.predict(X_test_red)
print("Red Wine - Confusion Matrix:")
print(confusion_matrix(y_test_red, y_pred_red))
print("\nRed Wine - Classification Report:")
print(classification_report(y_test_red, y_pred_red))

# White wine evaluation
y_pred_white = cart_model_white.predict(X_test_white)
print("White Wine - Confusion Matrix:")
print(confusion_matrix(y_test_white, y_pred_white))
print("\nWhite Wine - Classification Report:")
print(classification_report(y_test_white, y_pred_white))

#EXPT - 9 Implement ensemble learning models to perform classification.
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import classification_report, confusion_matrix

# Step 1: Load the datasets
red_wine = pd.read_csv('winequality-red.csv', sep=';')  # Ensure correct separator
white_wine = pd.read_csv('winequality-white.csv', sep=';')

# Step 2: Preprocess the data
def prepare_data(wine_data):
    X = wine_data.drop(columns=['quality'])
    y = wine_data['quality']
    return X, y

X_red, y_red = prepare_data(red_wine)
X_white, y_white = prepare_data(white_wine)

# Step 3: Split the data into training and testing sets
X_train_red, X_test_red, y_train_red, y_test_red = train_test_split(X_red, y_red, test_size=0.2, random_state=42)
X_train_white, X_test_white, y_train_white, y_test_white = train_test_split(X_white, y_white, test_size=0.2, random_state=42)

# Step 4: Train ensemble models

# Random Forest for Red Wine
rf_model_red = RandomForestClassifier(random_state=42)
rf_model_red.fit(X_train_red, y_train_red)

# Random Forest for White Wine
rf_model_white = RandomForestClassifier(random_state=42)
rf_model_white.fit(X_train_white, y_train_white)

# Gradient Boosting for Red Wine
gb_model_red = GradientBoostingClassifier(random_state=42)
gb_model_red.fit(X_train_red, y_train_red)

# Gradient Boosting for White Wine
gb_model_white = GradientBoostingClassifier(random_state=42)
gb_model_white.fit(X_train_white, y_train_white)

# AdaBoost for Red Wine
ada_model_red = AdaBoostClassifier(n_estimators=50, random_state=42)
ada_model_red.fit(X_train_red, y_train_red)

# AdaBoost for White Wine
ada_model_white = AdaBoostClassifier(n_estimators=50, random_state=42)
ada_model_white.fit(X_train_white, y_train_white)

# Step 5: Evaluate the models

# Function to evaluate model performance
def evaluate_model(model, X_test, y_test):
    y_pred = model.predict(X_test)
    print("Confusion Matrix:")
    print(confusion_matrix(y_test, y_pred))
    print("\nClassification Report:")
    print(classification_report(y_test, y_pred))

# Evaluate models
print("Random Forest - Red Wine Results:")
evaluate_model(rf_model_red, X_test_red, y_test_red)

print("Random Forest - White Wine Results:")
evaluate_model(rf_model_white, X_test_white, y_test_white)

print("Gradient Boosting - Red Wine Results:")
evaluate_model(gb_model_red, X_test_red, y_test_red)

print("Gradient Boosting - White Wine Results:")
evaluate_model(gb_model_white, X_test_white, y_test_white)

print("AdaBoost - Red Wine Results:")
evaluate_model(ada_model_red, X_test_red, y_test_red)

print("AdaBoost - White Wine Results:")
evaluate_model(ada_model_white, X_test_white, y_test_white)